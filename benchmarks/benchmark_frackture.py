#!/usr/bin/env python3
"""
Frackture Benchmark Suite
Compares Frackture compression/encryption/hash performance against gzip and brotli
over representative datasets.
"""

import sys
import os
import time
import json
import tracemalloc
import gzip
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Callable, Tuple, Optional
from dataclasses import dataclass, asdict
import random
import string
import hashlib
import secrets

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

# Import dataset repository
try:
    from dataset_repository import DatasetRepository
    HAS_DATASET_REPO = True
except ImportError:
    HAS_DATASET_REPO = False
    print("Warning: DatasetRepository not available, using legacy DatasetGenerator")

# Import Frackture - note the module name has spaces
import importlib.util
spec = importlib.util.spec_from_file_location("frackture", str(Path(__file__).parent.parent / "frackture (2).py"))
frackture_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(frackture_module)

frackture_preprocess_universal_v2_6 = frackture_module.frackture_preprocess_universal_v2_6
frackture_v3_3_safe = frackture_module.frackture_v3_3_safe
frackture_v3_3_reconstruct = frackture_module.frackture_v3_3_reconstruct
frackture_deterministic_hash = frackture_module.frackture_deterministic_hash
optimize_frackture = frackture_module.optimize_frackture
frackture_encrypt_payload = frackture_module.frackture_encrypt_payload
frackture_decrypt_payload = frackture_module.frackture_decrypt_payload
CompressionTier = frackture_module.CompressionTier
select_tier = frackture_module.select_tier

# New compact payload functions
FrackturePayload = frackture_module.FrackturePayload
serialize_frackture_payload = frackture_module.serialize_frackture_payload
deserialize_frackture_payload = frackture_module.deserialize_frackture_payload
compress_simple = frackture_module.compress_simple
decompress_simple = frackture_module.decompress_simple
compress_preset_tiny = frackture_module.compress_preset_tiny
compress_preset_default = frackture_module.compress_preset_default
compress_preset_large = frackture_module.compress_preset_large

# Try to import cryptography for AES-GCM
try:
    from cryptography.hazmat.primitives.ciphers.aead import AESGCM
    HAS_CRYPTOGRAPHY = True
except ImportError:
    HAS_CRYPTOGRAPHY = False
    print("Warning: cryptography not available. Install with: pip install cryptography")

# Try to import brotli
try:
    import brotli
    HAS_BROTLI = True
except ImportError:
    HAS_BROTLI = False
    print("Warning: brotli not available. Install with: pip install brotli")

# Try to import psutil for more accurate memory tracking
try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False


@dataclass
class BenchmarkResult:
    """Container for benchmark results"""

    name: str
    dataset_type: str
    original_size: int
    compressed_size: int
    compression_ratio: float
    encode_time: float
    decode_time: float
    encode_throughput: float  # MB/s
    decode_throughput: float  # MB/s
    hash_time: float
    peak_memory_mb: float
    success: bool
    sha256_time: Optional[float] = None
    error: str = ""

    # Competitor settings (for multi-level sweeps)
    gzip_level: Optional[int] = None
    brotli_quality: Optional[int] = None

    # New verification metrics
    symbolic_bytes: int = 0
    entropy_bytes: int = 0
    serialized_total_bytes: int = 0
    payload_is_96b: bool = False
    baseline_mse: float = 0.0
    optimized_mse: float = 0.0
    optimization_improvement_pct: float = 0.0
    optimization_trials: int = 0
    is_lossless: bool = False
    is_deterministic: bool = False
    determinism_drifts: int = 0
    fault_injection_passed: bool = False
    fault_injection_errors: Optional[List[str]] = None

    # Dataset metadata (tier/category)
    tier_name: Optional[str] = None
    category_name: Optional[str] = None
    actual_size_bytes: Optional[int] = None
    target_size_bytes: Optional[int] = None

    # Frackture internal compression tier (tiny/default/large)
    frackture_tier_name: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        base = asdict(self)
        for k, v in vars(self).items():
            if k not in base:
                base[k] = v
        return base


class DatasetGenerator:
    """Generate various types of test datasets"""
    
    @staticmethod
    def generate_text(size_kb: int = 100) -> bytes:
        """Generate Lorem Ipsum-style text"""
        words = ["lorem", "ipsum", "dolor", "sit", "amet", "consectetur", 
                 "adipiscing", "elit", "sed", "do", "eiusmod", "tempor",
                 "incididunt", "ut", "labore", "et", "dolore", "magna", "aliqua"]
        
        text = []
        target_size = size_kb * 1024
        while len(' '.join(text).encode()) < target_size:
            text.append(random.choice(words))
        
        return ' '.join(text).encode()[:target_size]
    
    @staticmethod
    def generate_json(size_kb: int = 100) -> bytes:
        """Generate structured JSON data"""
        data = {
            "users": [],
            "metadata": {
                "version": "1.0",
                "timestamp": "2024-01-01T00:00:00Z",
                "count": 0
            }
        }
        
        target_size = size_kb * 1024
        user_id = 0
        
        while len(json.dumps(data).encode()) < target_size:
            user = {
                "id": user_id,
                "name": f"User {user_id}",
                "email": f"user{user_id}@example.com",
                "age": random.randint(18, 80),
                "active": random.choice([True, False]),
                "tags": random.sample(["python", "javascript", "rust", "go", "java"], k=2)
            }
            data["users"].append(user)
            user_id += 1
        
        data["metadata"]["count"] = len(data["users"])
        return json.dumps(data).encode()[:target_size]
    
    @staticmethod
    def generate_binary_blob(size_kb: int = 100) -> bytes:
        """Generate mixed binary data (partially compressible)"""
        size = size_kb * 1024
        # Mix of repeated patterns and random data
        repeated = b'\x00\xFF\xAA\x55' * (size // 8)
        random_bytes = bytes(random.randint(0, 255) for _ in range(size // 4))
        data = repeated + random_bytes
        return data[:size]
    
    @staticmethod
    def generate_random_noise(size_kb: int = 100) -> bytes:
        """Generate random noise (incompressible)"""
        size = size_kb * 1024
        return bytes(random.randint(0, 255) for _ in range(size))
    
    @staticmethod
    def generate_highly_repetitive(size_kb: int = 100) -> bytes:
        """Generate highly repetitive data (very compressible)"""
        pattern = b"ABCD" * 256  # 1KB pattern
        size = size_kb * 1024
        return (pattern * (size // len(pattern) + 1))[:size]
    
    @staticmethod
    def generate_tiny_datasets() -> Dict[str, bytes]:
        """Generate tiny datasets (<100B) for edge case testing"""
        return {
            "tiny_text": b"Hello World! This is a tiny text dataset.",
            "tiny_json": b'{"tiny": true, "data": "minimal"}',
            "tiny_binary": b'\x00\x01\x02\x03\x04\x05\x06\x07\x08\x09',
            "tiny_random": bytes(random.randint(0, 255) for _ in range(50)),
            "tiny_repetitive": b"AB" * 25  # 50 bytes of ABABAB...
        }
    
    @staticmethod
    def generate_extreme_datasets() -> Dict[str, bytes]:
        """Generate extreme datasets for stress testing"""
        # Highly compressible - very repetitive
        highly_compressible = b"REPEAT_THIS_PATTERN_OVER_AND_OVER_AGAIN_AND_AGAIN" * 10000  # ~2MB
        extremely_compressible = b"X" * 104857600  # 100MB of same character
        
        # Random - incompressible
        extremely_random = bytes(random.randint(0, 255) for _ in range(104857600))  # 100MB random
        
        # Mixed patterns
        mixed_compressible = (b"PATTERN1" * 500000 + b"PATTERN2" * 500000)  # 8MB mixed
        mixed_random = bytes(random.randint(0, 255) for _ in range(10485760))  # 10MB random
        
        return {
            "highly_compressible": highly_compressible,
            "extremely_compressible": extremely_compressible,
            "extremely_random": extremely_random,
            "mixed_compressible": mixed_compressible,
            "mixed_random": mixed_random
        }
    
    @staticmethod
    def get_all_datasets(small: bool = True) -> Dict[str, bytes]:
        """Get all test datasets"""
        size = 100 if small else 1000  # 100KB or 1MB
        
        return {
            "text": DatasetGenerator.generate_text(size),
            "json": DatasetGenerator.generate_json(size),
            "binary_blob": DatasetGenerator.generate_binary_blob(size),
            "random_noise": DatasetGenerator.generate_random_noise(size),
            "highly_repetitive": DatasetGenerator.generate_highly_repetitive(size)
        }


class MemoryTracker:
    """Track memory usage during benchmarks"""
    
    def __init__(self):
        self.peak_memory = 0
        self.use_psutil = HAS_PSUTIL
        if self.use_psutil:
            self.process = psutil.Process()
    
    def start(self):
        """Start tracking memory"""
        tracemalloc.start()
        if self.use_psutil:
            self.process.memory_info()  # Warm up
    
    def stop(self) -> float:
        """Stop tracking and return peak memory in MB"""
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        
        # Use psutil if available for more accurate measurement
        if self.use_psutil:
            mem_info = self.process.memory_info()
            return mem_info.rss / (1024 * 1024)  # Convert to MB
        
        return peak / (1024 * 1024)  # Convert to MB


class BenchmarkRunner:
    """Run benchmarks for different compression methods"""

    _HASH_TARGET_TOTAL_BYTES = 2 * 1024 * 1024
    _HASH_MIN_RUNS = 5
    _HASH_MAX_RUNS = 200

    @staticmethod
    def _hash_runs_for_size(size_bytes: int) -> int:
        if size_bytes <= 0:
            return BenchmarkRunner._HASH_MIN_RUNS

        est = BenchmarkRunner._HASH_TARGET_TOTAL_BYTES // max(1, int(size_bytes))
        return int(max(BenchmarkRunner._HASH_MIN_RUNS, min(BenchmarkRunner._HASH_MAX_RUNS, est)))

    @staticmethod
    def _avg_latency_ms(fn: Callable[[], Any], *, runs: int) -> float:
        fn()  # warm-up
        start_ns = time.perf_counter_ns()
        for _ in range(runs):
            fn()
        elapsed_ns = time.perf_counter_ns() - start_ns
        return (elapsed_ns / runs) / 1_000_000.0

    @staticmethod
    def benchmark_frackture(data: bytes) -> BenchmarkResult:
        """Benchmark Frackture compression with comprehensive verification metrics"""
        dataset_type = "unknown"
        original_size = len(data)
        
        try:
            # Detect tier from input size
            tier = select_tier(data)
            
            # Memory tracking
            mem_tracker = MemoryTracker()
            mem_tracker.start()
            
            # Preprocessing with tier awareness
            preprocessed = frackture_preprocess_universal_v2_6(data, tier=tier)
            
            # === 1. BASELINE ENCODING/DECODING ===
            encode_start = time.perf_counter()
            payload = frackture_v3_3_safe(preprocessed, tier=tier)
            encode_time = time.perf_counter() - encode_start
            
            # === 2. PAYLOAD SIZING METRICS ===
            # Handle both dict (legacy) and FrackturePayload (new) formats
            if hasattr(payload, 'to_bytes'):  # It's a FrackturePayload
                # Use compact format for measurement
                compact_bytes = payload.to_bytes()
                serialized_total_bytes = len(compact_bytes)
                payload_is_96b = 90 <= serialized_total_bytes <= 102
                
                # Calculate symbolic and entropy bytes from compact format
                symbolic_bytes = 32  # Fixed 32 bytes in compact format
                entropy_bytes = 32   # 16 uint16 values = 32 bytes
            else:  # Legacy dict format
                # Calculate symbolic bytes (hex string length)
                symbolic_bytes = len(payload['symbolic']) if payload['symbolic'] else 0
                symbolic_bytes //= 2  # Convert hex chars to bytes (2 chars = 1 byte)
                
                # Calculate entropy bytes (16 floats * 8 bytes each for double precision)
                entropy_bytes = len(payload['entropy']) * 8
                
                # Calculate serialized total using compact format for comparison
                try:
                    from frackture_module import serialize_frackture_payload
                    compact_bytes = serialize_frackture_payload(payload)
                    serialized_total_bytes = len(compact_bytes)
                    payload_is_96b = 90 <= serialized_total_bytes <= 102
                except:
                    # Fallback to legacy JSON if compact serialization fails
                    serializable_payload = {
                        'symbolic': payload['symbolic'],
                        'entropy': [float(x) for x in payload['entropy']]
                    }
                    payload_bytes = json.dumps(serializable_payload).encode()
                    serialized_total_bytes = len(payload_bytes)
                    payload_is_96b = 90 <= serialized_total_bytes <= 102
            
            # === 3. RECONSTRUCTION QUALITY ===
            decode_start = time.perf_counter()
            reconstructed = frackture_v3_3_reconstruct(payload)
            decode_time = time.perf_counter() - decode_start
            
            # Calculate MSE between original and reconstructed
            baseline_mse = float(np.mean((preprocessed - reconstructed) ** 2))
            is_lossless = baseline_mse < 1e-6  # Threshold for lossless
            
            # === 4. OPTIMIZATION COMPARISON ===
            optimization_start = time.perf_counter()
            optimized_payload, optimized_mse = optimize_frackture(preprocessed, num_trials=5, tier=tier)
            optimization_time = time.perf_counter() - optimization_start

            # Calculate improvement percentage
            if baseline_mse > 0:
                optimization_improvement_pct = ((baseline_mse - optimized_mse) / baseline_mse) * 100
            else:
                optimization_improvement_pct = 0.0

            optimization_trials = 5 if tier != CompressionTier.TINY else 2
            
            # === 5. DETERMINISM VALIDATION ===
            # Encode the same input multiple times and check for identical payloads
            num_determinism_tests = 3
            deterministic_payloads = []
            for i in range(num_determinism_tests):
                test_payload = frackture_v3_3_safe(preprocessed)
                deterministic_payloads.append(test_payload)
            
            # Check if all payloads are identical
            is_deterministic = True
            determinism_drifts = 0
            if len(deterministic_payloads) > 1:
                first_payload = deterministic_payloads[0]
                for i in range(1, len(deterministic_payloads)):
                    if hasattr(first_payload, 'to_bytes'):  # FrackturePayload format
                        if first_payload.to_bytes() != deterministic_payloads[i].to_bytes():
                            is_deterministic = False
                            determinism_drifts += 1
                    else:  # Legacy dict format
                        if (first_payload['symbolic'] != deterministic_payloads[i]['symbolic'] or
                            first_payload['entropy'] != deterministic_payloads[i]['entropy']):
                            is_deterministic = False
                            determinism_drifts += 1
            
            # === 6. FAULT INJECTION ===
            fault_injection_passed = True
            fault_injection_errors = []

            def _expect_value_error(label: str, fn: Callable[[], None]) -> None:
                nonlocal fault_injection_passed, fault_injection_errors
                try:
                    fn()
                    fault_injection_passed = False
                    fault_injection_errors.append(f"{label} mutation not detected")
                except ValueError:
                    pass
                except Exception as e:
                    fault_injection_passed = False
                    fault_injection_errors.append(f"{label} raised unexpected {type(e).__name__}: {e}")

            try:
                import copy

                fault_injection_key = "benchmark_fault_injection_key"

                # Handle both payload formats for fault injection
                if hasattr(payload, 'to_bytes'):  # FrackturePayload format
                    payload_dict = payload.to_legacy_dict()
                    payload_dict.update({
                        "tier_name": "benchmark",
                        "category_name": "benchmark",
                        "actual_size_bytes": original_size,
                        "target_size_bytes": original_size,
                    })
                else:  # Legacy dict format
                    payload_dict = {
                        "symbolic": payload["symbolic"],
                        "entropy": payload["entropy"],
                        "tier_name": "benchmark",
                        "category_name": "benchmark",
                        "actual_size_bytes": original_size,
                        "target_size_bytes": original_size,
                    }

                # Raw payload tampering (structural corruption)
                mutated_symbolic = "".join("FF" if c != "F" else "00" for c in payload_dict["symbolic"])
                _expect_value_error(
                    "Raw symbolic",
                    lambda: frackture_v3_3_reconstruct({"symbolic": mutated_symbolic, "entropy": payload_dict["entropy"]}),
                )

                mutated_entropy = list(payload_dict["entropy"])
                mutated_entropy[0] = float("nan")
                _expect_value_error(
                    "Raw entropy",
                    lambda: frackture_v3_3_reconstruct({"symbolic": payload_dict["symbolic"], "entropy": mutated_entropy}),
                )

                _expect_value_error(
                    "Raw metadata",
                    lambda: frackture_v3_3_reconstruct({**payload_dict, "tier_name": 123}),
                )

                _expect_value_error("Raw empty", lambda: frackture_v3_3_reconstruct({}))

                _expect_value_error(
                    "Raw invalid hex",
                    lambda: frackture_v3_3_reconstruct({"symbolic": "INVALID_HEX_!@#$%", "entropy": payload_dict["entropy"]}),
                )

                # Encrypted payload tampering (integrity corruption)
                encrypted = frackture_encrypt_payload(payload_dict, fault_injection_key)

                sym = encrypted["data"]["symbolic"]
                flipped_sym = ("0" if sym[0] != "0" else "1") + sym[1:]
                tampered = copy.deepcopy(encrypted)
                tampered["data"]["symbolic"] = flipped_sym
                _expect_value_error(
                    "Encrypted symbolic",
                    lambda: frackture_decrypt_payload(tampered, fault_injection_key),
                )

                tampered = copy.deepcopy(encrypted)
                tampered["data"]["entropy"][0] = float(tampered["data"]["entropy"][0]) + 1.0
                _expect_value_error(
                    "Encrypted entropy",
                    lambda: frackture_decrypt_payload(tampered, fault_injection_key),
                )

                tampered = copy.deepcopy(encrypted)
                tampered["data"]["tier_name"] = "tampered"
                _expect_value_error(
                    "Encrypted payload metadata",
                    lambda: frackture_decrypt_payload(tampered, fault_injection_key),
                )

                tampered = copy.deepcopy(encrypted)
                tampered["metadata"]["key_id"] = "deadbeef"
                _expect_value_error(
                    "Encrypted envelope metadata",
                    lambda: frackture_decrypt_payload(tampered, fault_injection_key),
                )

                tampered = copy.deepcopy(encrypted)
                tampered["signature"] = "0" * 64
                _expect_value_error(
                    "Encrypted signature",
                    lambda: frackture_decrypt_payload(tampered, fault_injection_key),
                )

            except Exception as e:
                fault_injection_passed = False
                fault_injection_errors.append(f"Fault injection test error: {str(e)}")
            
            # === 7. TIMING COMPLETION ===
            hash_runs = BenchmarkRunner._hash_runs_for_size(original_size)
            hash_time = BenchmarkRunner._avg_latency_ms(
                lambda: frackture_deterministic_hash(data),
                runs=hash_runs,
            )
            
            # Baseline SHA256 timing for comparison
            sha256_time = BenchmarkRunner._avg_latency_ms(
                lambda: hashlib.sha256(data).hexdigest(),
                runs=hash_runs,
            )
            
            # Stop memory tracking
            peak_memory = mem_tracker.stop()
            
            # === 8. FINAL METRICS CALCULATION ===
            # Use baseline encoding time for throughput (exclude optimization)
            compression_ratio = original_size / serialized_total_bytes if serialized_total_bytes > 0 else 0
            encode_throughput = (original_size / (1024 * 1024)) / encode_time if encode_time > 0 else 0
            decode_throughput = (original_size / (1024 * 1024)) / decode_time if decode_time > 0 else 0
            
            return BenchmarkResult(
                name="Frackture",
                dataset_type=dataset_type,
                original_size=original_size,
                compressed_size=serialized_total_bytes,
                compression_ratio=compression_ratio,
                encode_time=encode_time,
                decode_time=decode_time,
                encode_throughput=encode_throughput,
                decode_throughput=decode_throughput,
                hash_time=hash_time,
                sha256_time=sha256_time,
                peak_memory_mb=peak_memory,
                success=True,
                # New verification metrics
                symbolic_bytes=symbolic_bytes,
                entropy_bytes=entropy_bytes,
                serialized_total_bytes=serialized_total_bytes,
                payload_is_96b=payload_is_96b,
                baseline_mse=baseline_mse,
                optimized_mse=optimized_mse,
                optimization_improvement_pct=optimization_improvement_pct,
                optimization_trials=optimization_trials,
                is_lossless=is_lossless,
                is_deterministic=is_deterministic,
                determinism_drifts=determinism_drifts,
                fault_injection_passed=fault_injection_passed,
                fault_injection_errors=fault_injection_errors or [],
                frackture_tier_name=tier.value if tier else "default"
            )
            
        except Exception as e:
            return BenchmarkResult(
                name="Frackture",
                dataset_type=dataset_type,
                original_size=original_size,
                compressed_size=0,
                compression_ratio=0,
                encode_time=0,
                decode_time=0,
                encode_throughput=0,
                decode_throughput=0,
                hash_time=0,
                peak_memory_mb=0,
                success=False,
                error=str(e),
                # Default values for new metrics
                symbolic_bytes=0,
                entropy_bytes=0,
                serialized_total_bytes=0,
                payload_is_96b=False,
                baseline_mse=0.0,
                optimized_mse=0.0,
                optimization_improvement_pct=0.0,
                optimization_trials=0,
                is_lossless=False,
                is_deterministic=False,
                determinism_drifts=0,
                fault_injection_passed=False,
                fault_injection_errors=[]
            )
    
    @staticmethod
    def benchmark_gzip(data: bytes, level: int = 6) -> BenchmarkResult:
        """Benchmark gzip compression"""
        dataset_type = "unknown"
        original_size = len(data)
        
        try:
            mem_tracker = MemoryTracker()
            mem_tracker.start()
            
            # Encode timing
            encode_start = time.perf_counter()
            compressed = gzip.compress(data, compresslevel=level)
            encode_time = time.perf_counter() - encode_start
            
            compressed_size = len(compressed)
            
            # Decode timing
            decode_start = time.perf_counter()
            decompressed = gzip.decompress(compressed)
            decode_time = time.perf_counter() - decode_start
            
            # Hash timing (using same hash as Frackture for comparison)
            hash_runs = BenchmarkRunner._hash_runs_for_size(original_size)
            hash_time = BenchmarkRunner._avg_latency_ms(
                lambda: frackture_deterministic_hash(data),
                runs=hash_runs,
            )
            
            peak_memory = mem_tracker.stop()
            
            # Verify
            assert decompressed == data, "Decompression mismatch"
            
            compression_ratio = original_size / compressed_size if compressed_size > 0 else 0
            encode_throughput = (original_size / (1024 * 1024)) / encode_time if encode_time > 0 else 0
            decode_throughput = (original_size / (1024 * 1024)) / decode_time if decode_time > 0 else 0
            
            return BenchmarkResult(
                name=f"Gzip L{level}",
                dataset_type=dataset_type,
                original_size=original_size,
                compressed_size=compressed_size,
                compression_ratio=compression_ratio,
                encode_time=encode_time,
                decode_time=decode_time,
                encode_throughput=encode_throughput,
                decode_throughput=decode_throughput,
                hash_time=hash_time,
                peak_memory_mb=peak_memory,
                success=True,
                gzip_level=level
            )
            
        except Exception as e:
            return BenchmarkResult(
                name=f"Gzip L{level}",
                dataset_type=dataset_type,
                original_size=original_size,
                compressed_size=0,
                compression_ratio=0,
                encode_time=0,
                decode_time=0,
                encode_throughput=0,
                decode_throughput=0,
                hash_time=0,
                peak_memory_mb=0,
                success=False,
                error=str(e),
                gzip_level=level
            )
    
    @staticmethod
    def benchmark_brotli(data: bytes, quality: int = 6) -> BenchmarkResult:
        """Benchmark brotli compression"""
        if not HAS_BROTLI:
            return BenchmarkResult(
                name=f"Brotli Q{quality}",
                dataset_type="unknown",
                original_size=len(data),
                compressed_size=0,
                compression_ratio=0,
                encode_time=0,
                decode_time=0,
                encode_throughput=0,
                decode_throughput=0,
                hash_time=0,
                peak_memory_mb=0,
                success=False,
                error="Brotli not installed",
                brotli_quality=quality
            )
        
        dataset_type = "unknown"
        original_size = len(data)
        
        try:
            mem_tracker = MemoryTracker()
            mem_tracker.start()
            
            # Encode timing
            encode_start = time.perf_counter()
            compressed = brotli.compress(data, quality=quality)
            encode_time = time.perf_counter() - encode_start
            
            compressed_size = len(compressed)
            
            # Decode timing
            decode_start = time.perf_counter()
            decompressed = brotli.decompress(compressed)
            decode_time = time.perf_counter() - decode_start
            
            # Hash timing
            hash_runs = BenchmarkRunner._hash_runs_for_size(original_size)
            hash_time = BenchmarkRunner._avg_latency_ms(
                lambda: frackture_deterministic_hash(data),
                runs=hash_runs,
            )
            
            peak_memory = mem_tracker.stop()
            
            # Verify
            assert decompressed == data, "Decompression mismatch"
            
            compression_ratio = original_size / compressed_size if compressed_size > 0 else 0
            encode_throughput = (original_size / (1024 * 1024)) / encode_time if encode_time > 0 else 0
            decode_throughput = (original_size / (1024 * 1024)) / decode_time if decode_time > 0 else 0
            
            return BenchmarkResult(
                name=f"Brotli Q{quality}",
                dataset_type=dataset_type,
                original_size=original_size,
                compressed_size=compressed_size,
                compression_ratio=compression_ratio,
                encode_time=encode_time,
                decode_time=decode_time,
                encode_throughput=encode_throughput,
                decode_throughput=decode_throughput,
                hash_time=hash_time,
                peak_memory_mb=peak_memory,
                success=True,
                brotli_quality=quality
            )
            
        except Exception as e:
            return BenchmarkResult(
                name=f"Brotli Q{quality}",
                dataset_type=dataset_type,
                original_size=original_size,
                compressed_size=0,
                compression_ratio=0,
                encode_time=0,
                decode_time=0,
                encode_throughput=0,
                decode_throughput=0,
                hash_time=0,
                peak_memory_mb=0,
                success=False,
                error=str(e),
                brotli_quality=quality
            )
            
    @staticmethod
    def benchmark_sha256(data: bytes) -> BenchmarkResult:
        """Benchmark SHA256 hashing"""
        dataset_type = "unknown"
        original_size = len(data)
        
        try:
            mem_tracker = MemoryTracker()
            mem_tracker.start()
            
            hash_runs = BenchmarkRunner._hash_runs_for_size(original_size)
            avg_ms = BenchmarkRunner._avg_latency_ms(lambda: hashlib.sha256(data).hexdigest(), runs=hash_runs)

            # Encode timing (Hashing)
            encode_time = avg_ms / 1000.0
            digest = hashlib.sha256(data).hexdigest()

            compressed_size = len(digest)

            # Decode timing (None, irreversible)
            decode_time = 0.0

            # Hash timing (Self)
            hash_time = avg_ms

            peak_memory = mem_tracker.stop()
            
            compression_ratio = original_size / compressed_size if compressed_size > 0 else 0
            encode_throughput = (original_size / (1024 * 1024)) / encode_time if encode_time > 0 else 0
            decode_throughput = 0.0
            
            return BenchmarkResult(
                name="SHA256",
                dataset_type=dataset_type,
                original_size=original_size,
                compressed_size=compressed_size,
                compression_ratio=compression_ratio,
                encode_time=encode_time,
                decode_time=decode_time,
                encode_throughput=encode_throughput,
                decode_throughput=decode_throughput,
                hash_time=hash_time,
                peak_memory_mb=peak_memory,
                success=True
            )
            
        except Exception as e:
            return BenchmarkResult(
                name="SHA256",
                dataset_type=dataset_type,
                original_size=original_size,
                compressed_size=0,
                compression_ratio=0,
                encode_time=0,
                decode_time=0,
                encode_throughput=0,
                decode_throughput=0,
                hash_time=0,
                peak_memory_mb=0,
                success=False,
                error=str(e)
            )

    @staticmethod
    def benchmark_aes_gcm(data: bytes) -> BenchmarkResult:
        """Benchmark AES-GCM encryption/decryption"""
        if not HAS_CRYPTOGRAPHY:
            return BenchmarkResult(
                name="AES-GCM",
                dataset_type="unknown",
                original_size=len(data),
                compressed_size=0,
                compression_ratio=0,
                encode_time=0,
                decode_time=0,
                encode_throughput=0,
                decode_throughput=0,
                hash_time=0,
                peak_memory_mb=0,
                success=False,
                error="cryptography not installed"
            )
            
        dataset_type = "unknown"
        original_size = len(data)
        
        try:
            mem_tracker = MemoryTracker()
            mem_tracker.start()
            
            # Key generation (excluded from timing, one-time setup)
            key = AESGCM.generate_key(bit_length=256)
            aesgcm = AESGCM(key)
            nonce = secrets.token_bytes(12)
            
            # Encode timing (Encrypt)
            encode_start = time.perf_counter()
            ciphertext = aesgcm.encrypt(nonce, data, None)
            encode_time = time.perf_counter() - encode_start
            
            compressed_size = len(ciphertext)
            
            # Decode timing (Decrypt)
            decode_start = time.perf_counter()
            decrypted = aesgcm.decrypt(nonce, ciphertext, None)
            decode_time = time.perf_counter() - decode_start
            
            assert decrypted == data, "Decryption mismatch"
            
            # Hash timing (using Frackture hash for consistency)
            hash_runs = BenchmarkRunner._hash_runs_for_size(original_size)
            hash_time = BenchmarkRunner._avg_latency_ms(
                lambda: frackture_deterministic_hash(data),
                runs=hash_runs,
            )
            
            peak_memory = mem_tracker.stop()
            
            compression_ratio = original_size / compressed_size if compressed_size > 0 else 0
            encode_throughput = (original_size / (1024 * 1024)) / encode_time if encode_time > 0 else 0
            decode_throughput = (original_size / (1024 * 1024)) / decode_time if decode_time > 0 else 0
            
            return BenchmarkResult(
                name="AES-GCM",
                dataset_type=dataset_type,
                original_size=original_size,
                compressed_size=compressed_size,
                compression_ratio=compression_ratio,
                encode_time=encode_time,
                decode_time=decode_time,
                encode_throughput=encode_throughput,
                decode_throughput=decode_throughput,
                hash_time=hash_time,
                peak_memory_mb=peak_memory,
                success=True
            )
            
        except Exception as e:
            return BenchmarkResult(
                name="AES-GCM",
                dataset_type=dataset_type,
                original_size=original_size,
                compressed_size=0,
                compression_ratio=0,
                encode_time=0,
                decode_time=0,
                encode_throughput=0,
                decode_throughput=0,
                hash_time=0,
                peak_memory_mb=0,
                success=False,
                error=str(e)
            )

    @staticmethod
    def benchmark_frackture_encryption(data: bytes) -> BenchmarkResult:
        """Benchmark Frackture HMAC Encryption"""
        dataset_type = "unknown"
        original_size = len(data)
        
        try:
            mem_tracker = MemoryTracker()
            mem_tracker.start()
            
            # Preprocessing (needed for payload generation)
            # We measure from payload generation to encrypted payload?
            # Or just the encryption wrapper overhead?
            # "extend the existing frackture encryption (HMAC) path so it is benchmarked separately with the same metrics"
            # I assume we benchmark the whole process or just the encrypt/decrypt wrapper on top of payload.
            # But AES-GCM benchmarks encrypting the RAW data. Frackture encryption encrypts the PAYLOAD.
            # So to compare, we should probably follow the Frackture flow:
            # Data -> Payload -> Encrypted Payload
            
            # Preprocessing & Payload Generation (Setup, excluded from encrypt time? or included?)
            # If we compare to AES-GCM (Data -> Ciphertext), we might want Data -> Encrypted Payload.
            # BUT Frackture is compression + encryption.
            # Let's measure the ENCRYPTION step specifically, but output the total size.
            # However, the user wants to benchmark the "frackture encryption path".
            # The most fair comparison for "Encryption" is taking the payload and signing it.
            # But wait, `frackture_encrypt_payload` takes a payload (dict).
            
            # Let's generate the payload first.
            preprocessed = frackture_preprocess_universal_v2_6(data)
            payload = frackture_v3_3_safe(preprocessed)
            key = secrets.token_hex(32) # 64 hex chars = 32 bytes
            
            # Encode timing (Encrypt Payload)
            encode_start = time.perf_counter()
            encrypted_payload = frackture_encrypt_payload(payload, key)
            encode_time = time.perf_counter() - encode_start
            
            # Size of encrypted payload
            import json
            encrypted_bytes = json.dumps(encrypted_payload).encode()
            compressed_size = len(encrypted_bytes)
            
            # Decode timing (Decrypt/Verify Payload)
            decode_start = time.perf_counter()
            decrypted_payload = frackture_decrypt_payload(encrypted_payload, key)
            decode_time = time.perf_counter() - decode_start
            
            # Hash timing
            hash_runs = BenchmarkRunner._hash_runs_for_size(original_size)
            hash_time = BenchmarkRunner._avg_latency_ms(
                lambda: frackture_deterministic_hash(data),
                runs=hash_runs,
            )
            
            peak_memory = mem_tracker.stop()
            
            # For throughput, we use original_size, even though we encrypt payload.
            # This represents "how fast can I process X MB of original data securely"
            # But wait, if we only measure encrypt step time, the throughput will be huge because payload is small (96B).
            # If we want comparable "System Throughput", we should include payload generation time?
            # The ticket says: "extend the existing frackture encryption (HMAC) path so it is benchmarked separately"
            # If I benchmark AES-GCM, I do Data -> Ciphertext.
            # If I benchmark Frackture Encryption, is it Data -> Encrypted Payload?
            # If so, I should include payload generation time.
            # Let's check `benchmark_frackture`. It measures `frackture_v3_3_safe` which is Data -> Payload.
            # So `benchmark_frackture_encryption` should probably be Data -> Encrypted Payload.
            
            # Let's re-measure to include payload generation in encode_time.
            # BUT `benchmark_frackture` already exists.
            # If I add `benchmark_frackture_encryption`, it will be a separate row.
            # If I include payload generation, it will be slightly slower than `benchmark_frackture`.
            # If I EXCLUDE payload generation, it will be insanely fast (signing 96 bytes).
            
            # Given the context of "crypto/hash baselines", it seems they want to compare:
            # 1. SHA256 (Hashing)
            # 2. AES-GCM (Encryption)
            # 3. Frackture Encryption (The encryption component of Frackture)
            
            # If I want to compare "Frackture Encryption" vs "AES-GCM", I should probably compare the mechanism itself.
            # However, Frackture Encryption is applied ON TOP of compression.
            # If I apply AES-GCM on original data, I get large ciphertext.
            # If I apply Frackture Encryption, I get small authenticated payload.
            
            # I will measure the FULL path: Data -> Preprocess -> Payload -> Encrypt.
            # This shows the user the "Secure Frackture" performance.
            
            # Redo timing:
            
            encode_start = time.perf_counter()
            # 1. Preprocess
            preprocessed_t = frackture_preprocess_universal_v2_6(data)
            # 2. Payload
            payload_t = frackture_v3_3_safe(preprocessed_t)
            # 3. Encrypt
            encrypted_payload = frackture_encrypt_payload(payload_t, key)
            encode_time = time.perf_counter() - encode_start
            
            # Redo decode timing:
            decode_start = time.perf_counter()
            # 1. Decrypt/Verify
            decrypted_payload_t = frackture_decrypt_payload(encrypted_payload, key)
            # 2. Reconstruct
            reconstructed = frackture_v3_3_reconstruct(decrypted_payload_t)
            decode_time = time.perf_counter() - decode_start
            
            compression_ratio = original_size / compressed_size if compressed_size > 0 else 0
            encode_throughput = (original_size / (1024 * 1024)) / encode_time if encode_time > 0 else 0
            decode_throughput = (original_size / (1024 * 1024)) / decode_time if decode_time > 0 else 0

            return BenchmarkResult(
                name="Frackture Encrypted",
                dataset_type=dataset_type,
                original_size=original_size,
                compressed_size=compressed_size,
                compression_ratio=compression_ratio,
                encode_time=encode_time,
                decode_time=decode_time,
                encode_throughput=encode_throughput,
                decode_throughput=decode_throughput,
                hash_time=hash_time,
                peak_memory_mb=peak_memory,
                success=True,
                # Include verification metrics from the payload
                symbolic_bytes=len(payload_t['symbolic'])//2 if payload_t['symbolic'] else 0,
                entropy_bytes=len(payload_t['entropy'])*8,
                serialized_total_bytes=compressed_size,
                payload_is_96b=False, # It will be larger due to wrapper
                is_lossless=False, # Frackture is lossy
                is_deterministic=True # Should be
            )
            
        except Exception as e:
            return BenchmarkResult(
                name="Frackture Encrypted",
                dataset_type=dataset_type,
                original_size=original_size,
                compressed_size=0,
                compression_ratio=0,
                encode_time=0,
                decode_time=0,
                encode_throughput=0,
                decode_throughput=0,
                hash_time=0,
                peak_memory_mb=0,
                success=False,
                error=str(e)
            )



class ResultFormatter:
    """Format and display benchmark results"""
    
    @staticmethod
    def format_size(size_bytes: int) -> str:
        """Format bytes to human-readable size"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.2f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.2f} TB"
    
    @staticmethod
    def print_table(results: List[BenchmarkResult], dataset_name: str):
        """Print results as a formatted table with enhanced verification metrics"""
        print(f"\n{'='*140}")
        print(f"Dataset: {dataset_name}")
        print(f"{'='*140}")
        
        if not results:
            print("No results to display")
            return
        
        # Check if we have Frackture results to determine header format
        has_frackture = any(r.name == "Frackture" and r.success for r in results)
        
        if has_frackture:
            # Enhanced header for Frackture with verification metrics
            headers = [
                "Method",
                "Original",
                "Compressed",
                "Ratio",
                "Encode (ms)",
                "Decode (ms)",
                "Enc Speed",
                "Dec Speed",
                "Hash (ms)",
                "Mem (MB)",
                "Payload (B)",
                "96B?",
                "MSE Baseline",
                "MSE Optimized",
                "Lossless?",
                "Deterministic?"
            ]
        else:
            # Standard header for other methods
            headers = [
                "Method",
                "Original",
                "Compressed",
                "Ratio",
                "Encode (ms)",
                "Decode (ms)",
                "Enc Speed",
                "Dec Speed",
                "Hash (ms)",
                "Mem (MB)",
                "-",
                "-",
                "-",
                "-",
                "-",
                "-"
            ]
        
        # Column widths
        widths = [20, 12, 12, 8, 12, 12, 12, 12, 10, 10, 12, 8, 12, 12, 12, 14]
        
        # Print header
        header_line = "| " + " | ".join(
            h.ljust(w) for h, w in zip(headers, widths)
        ) + " |"
        print(header_line)
        print("|" + "|".join("-" * (w + 2) for w in widths) + "|")
        
        # Print rows
        for result in results:
            if not result.success:
                row = [
                    result.name,
                    ResultFormatter.format_size(result.original_size),
                    "FAILED",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-"
                ]
            elif result.name == "Frackture":
                # Frackture gets enhanced metrics
                row = [
                    result.name,
                    ResultFormatter.format_size(result.original_size),
                    ResultFormatter.format_size(result.compressed_size),
                    f"{result.compression_ratio:.2f}x",
                    f"{result.encode_time * 1000:.2f}",
                    f"{result.decode_time * 1000:.2f}",
                    f"{result.encode_throughput:.2f} MB/s",
                    f"{result.decode_throughput:.2f} MB/s",
                    f"{result.hash_time:.4f}",
                    f"{result.peak_memory_mb:.2f}",
                    f"{result.serialized_total_bytes}",
                    "âœ“" if result.payload_is_96b else "âœ—",
                    f"{result.baseline_mse:.6f}",
                    f"{result.optimized_mse:.6f}",
                    "âœ“" if result.is_lossless else "âœ—",
                    "âœ“" if result.is_deterministic else f"Drift({result.determinism_drifts})"
                ]
            else:
                # Standard methods get standard metrics
                row = [
                    result.name,
                    ResultFormatter.format_size(result.original_size),
                    ResultFormatter.format_size(result.compressed_size),
                    f"{result.compression_ratio:.2f}x",
                    f"{result.encode_time * 1000:.2f}",
                    f"{result.decode_time * 1000:.2f}",
                    f"{result.encode_throughput:.2f} MB/s",
                    f"{result.decode_throughput:.2f} MB/s",
                    f"{result.hash_time:.4f}",
                    f"{result.peak_memory_mb:.2f}",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-"
                ]
            row_line = "| " + " | ".join(
                str(v).ljust(w) for v, w in zip(row, widths)
            ) + " |"
            print(row_line)
        
        # Print Frackture-specific verification details
        for result in results:
            if result.name == "Frackture" and result.success:
                print(f"\nðŸ” Frackture Verification Details for {dataset_name}:")
                print(f"  ðŸ“Š Payload Sizing: Symbolic={result.symbolic_bytes}B, Entropy={result.entropy_bytes}B, Total={result.serialized_total_bytes}B")
                print(f"  ðŸŽ¯ 96B Target: {'âœ… Met' if result.payload_is_96b else 'âŒ Missed'} (target: ~96B)")
                print(f"  ðŸ“ˆ Optimization: {result.optimization_improvement_pct:.2f}% improvement over {result.optimization_trials} trials")
                print(f"  ðŸ”§ Fault Injection: {'âœ… Passed' if result.fault_injection_passed else 'âŒ Failed'}")
                if result.fault_injection_errors:
                    print(f"  âš ï¸  Fault Errors: {', '.join(result.fault_injection_errors)}")
        
        print(f"{'='*140}\n")
    
    @staticmethod
    def save_json(
        results_by_dataset: Dict[str, List[BenchmarkResult]],
        output_path: Path,
        *,
        competition_summary: Optional[Dict[str, Any]] = None,
        competition_records: Optional[Dict[str, Any]] = None,
        benchmark_config: Optional[Dict[str, Any]] = None,
    ):
        """Save results as JSON with enhanced verification metrics"""
        output = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "benchmark_version": "2.1.0",
            "enhanced_metrics": {
                "payload_sizing": "Symbolic bytes, entropy bytes, serialized total, 96B validation",
                "reconstruction_quality": "MSE baseline vs optimized, lossless status",
                "optimization": "MSE improvement percentage, trials count",
                "determinism": "Multiple encoding tests, drift detection",
                "fault_injection": "Payload mutation tests, error handling validation",
                "competition": "Frackture vs Gzip/Brotli per-config and per-tier win/loss summary",
            },
            "benchmark_config": benchmark_config or {},
            "competition_summary": competition_summary or {},
            "competition_records": competition_records or {},
            "results": {},
        }
        
        for dataset_name, results in results_by_dataset.items():
            output["results"][dataset_name] = [
                r.to_dict() for r in results
            ]
        
        with open(output_path, 'w') as f:
            json.dump(output, f, indent=2, default=str)
        
        print(f"âœ… JSON results saved to: {output_path}")
        print(f"ðŸ“Š Enhanced metrics version: {output['benchmark_version']}")
    
    @staticmethod
    def save_markdown(
        results_by_dataset: Dict[str, List[BenchmarkResult]],
        output_path: Path,
        *,
        competition_summary: Optional[Dict[str, Any]] = None,
        benchmark_config: Optional[Dict[str, Any]] = None,
    ):
        """Save results as Markdown with enhanced verification metrics"""
        lines = [
            "# Frackture Benchmark Results",
            "",
            f"**Generated:** {time.strftime('%Y-%m-%d %H:%M:%S')}",
            f"**Enhanced Metrics Version:** 2.1.0",
            "",
            "## New Verification Metrics",
            "",
            "- **Payload Sizing**: Symbolic bytes, entropy bytes, serialized total, 96B validation",
            "- **Reconstruction Quality**: MSE baseline vs optimized, lossless status",
            "- **Optimization**: MSE improvement percentage, trials count",
            "- **Determinism**: Multiple encoding tests, drift detection",
            "- **Fault Injection**: Payload mutation tests, error handling validation",
            "- **Competition Summary**: Frackture vs Gzip/Brotli wins by tier and by configuration",
            "",
        ]

        if benchmark_config:
            lines.extend([
                "## Benchmark Configuration",
                "",
                f"- **Gzip levels:** {benchmark_config.get('gzip_levels', [])}",
                f"- **Brotli qualities:** {benchmark_config.get('brotli_qualities', [])}",
                f"- **Real datasets:** {benchmark_config.get('use_real_datasets', False)}",
                f"- **All tiers:** {benchmark_config.get('all_tiers', False)}",
                "",
            ])

        overall_competition = (competition_summary or {}).get('overall') if competition_summary else None
        if overall_competition and overall_competition.get('total_comparisons', 0) > 0:
            lines.extend([
                "## Competition Summary (Frackture vs Gzip/Brotli)",
                "",
                "| Scope | Total Comparisons | Wins (Ratio) | Win Rate (Ratio) | Wins (Throughput) | Win Rate (Throughput) |",
                "|---|---:|---:|---:|---:|---:|",
                f"| Overall | {overall_competition.get('total_comparisons', 0)} | {overall_competition.get('frackture_wins_ratio', 0)} | {overall_competition.get('win_rate_ratio', 0.0) * 100:.1f}% | {overall_competition.get('frackture_wins_throughput', 0)} | {overall_competition.get('win_rate_throughput', 0.0) * 100:.1f}% |",
                "",
            ])

            by_tier = (competition_summary or {}).get('by_tier', {})
            if by_tier:
                lines.extend([
                    "### Win Rates by Tier",
                    "",
                    "| Tier | Total Comparisons | Wins (Ratio) | Win Rate (Ratio) |",
                    "|---|---:|---:|---:|",
                ])
                for tier_name, bucket in sorted(by_tier.items()):
                    total = bucket.get('total_comparisons', 0)
                    if total == 0:
                        continue
                    lines.append(
                        f"| {tier_name} | {total} | {bucket.get('frackture_wins_ratio', 0)} | {bucket.get('win_rate_ratio', 0.0) * 100:.1f}% |"
                    )
                lines.append("")

        lines.extend([
            "---",
            "",
        ])
        
        for dataset_name, results in results_by_dataset.items():
            lines.append(f"## Dataset: {dataset_name}")
            lines.append("")
            
            if not results:
                lines.append("*No results*")
                lines.append("")
                continue
            
            # Check if we have Frackture results for enhanced table
            has_frackture = any(r.name == "Frackture" and r.success for r in results)
            
            if has_frackture:
                # Enhanced table for Frackture
                lines.append("### Performance & Verification Metrics")
                lines.append("")
                lines.append("| Method | Original | Compressed | Ratio | Encode (ms) | Decode (ms) | Enc Speed | Dec Speed | Hash (ms) | Mem (MB) | Payload (B) | 96B? | MSE Baseline | MSE Optimized | Lossless? | Deterministic? |")
                lines.append("|--------|----------|------------|-------|-------------|-------------|-----------|-----------|-----------|----------|-------------|------|--------------|---------------|-----------|-----------------|")
                
                for result in results:
                    if not result.success:
                        lines.append(f"| {result.name} | {ResultFormatter.format_size(result.original_size)} | FAILED | - | - | - | - | - | - | - | - | - | - | - | - | - |")
                    elif result.name == "Frackture":
                        row = (
                            f"| {result.name} "
                            f"| {ResultFormatter.format_size(result.original_size)} "
                            f"| {ResultFormatter.format_size(result.compressed_size)} "
                            f"| {result.compression_ratio:.2f}x "
                            f"| {result.encode_time * 1000:.2f} "
                            f"| {result.decode_time * 1000:.2f} "
                            f"| {result.encode_throughput:.2f} MB/s "
                            f"| {result.decode_throughput:.2f} MB/s "
                            f"| {result.hash_time:.4f} "
                            f"| {result.peak_memory_mb:.2f} "
                            f"| {result.serialized_total_bytes} "
                            f"| {'âœ…' if result.payload_is_96b else 'âŒ'} "
                            f"| {result.baseline_mse:.6f} "
                            f"| {result.optimized_mse:.6f} "
                            f"| {'âœ…' if result.is_lossless else 'âŒ'} "
                            f"| {'âœ…' if result.is_deterministic else f'Drift({result.determinism_drifts})'} |"
                        )
                        lines.append(row)
                    else:
                        lines.append(f"| {result.name} | {ResultFormatter.format_size(result.original_size)} | {ResultFormatter.format_size(result.compressed_size)} | {result.compression_ratio:.2f}x | {result.encode_time * 1000:.2f} | {result.decode_time * 1000:.2f} | {result.encode_throughput:.2f} MB/s | {result.decode_throughput:.2f} MB/s | {result.hash_time:.4f} | {result.peak_memory_mb:.2f} | - | - | - | - | - | - |")
            else:
                # Standard table for non-Frackture results
                lines.append("| Method | Original | Compressed | Ratio | Encode (ms) | Decode (ms) | Enc Speed | Dec Speed | Hash (ms) | Mem (MB) |")
                lines.append("|--------|----------|------------|-------|-------------|-------------|-----------|-----------|-----------|----------|")
                
                for result in results:
                    if not result.success:
                        lines.append(f"| {result.name} | {ResultFormatter.format_size(result.original_size)} | FAILED | - | - | - | - | - | - | - |")
                    else:
                        row = (
                            f"| {result.name} "
                            f"| {ResultFormatter.format_size(result.original_size)} "
                            f"| {ResultFormatter.format_size(result.compressed_size)} "
                            f"| {result.compression_ratio:.2f}x "
                            f"| {result.encode_time * 1000:.2f} "
                            f"| {result.decode_time * 1000:.2f} "
                            f"| {result.encode_throughput:.2f} MB/s "
                            f"| {result.decode_throughput:.2f} MB/s "
                            f"| {result.hash_time:.4f} "
                            f"| {result.peak_memory_mb:.2f} |"
                        )
                        lines.append(row)
            
            # Add detailed verification metrics for Frackture
            for result in results:
                if result.name == "Frackture" and result.success:
                    lines.append("")
                    lines.append("### Frackture Verification Details")
                    lines.append("")
                    lines.append(f"- **Payload Sizing**: Symbolic={result.symbolic_bytes}B, Entropy={result.entropy_bytes}B, Total={result.serialized_total_bytes}B")
                    lines.append(f"- **96B Target**: {'âœ… Met' if result.payload_is_96b else 'âŒ Missed'} (target: ~96B)")
                    lines.append(f"- **Optimization**: {result.optimization_improvement_pct:.2f}% improvement over {result.optimization_trials} trials")
                    lines.append(f"- **Reconstruction**: {'âœ… Lossless' if result.is_lossless else 'âŒ Lossy'} (MSE: {result.baseline_mse:.6f})")
                    lines.append(f"- **Determinism**: {'âœ… Deterministic' if result.is_deterministic else f'âŒ Drift detected ({result.determinism_drifts} differences)'}")
                    lines.append(f"- **Fault Injection**: {'âœ… Passed' if result.fault_injection_passed else 'âŒ Failed'}")
                    if result.fault_injection_errors:
                        lines.append(f"- **Fault Errors**: {', '.join(result.fault_injection_errors)}")
            
            lines.append("")
            lines.append("---")
            lines.append("")
        
        # Add enhanced summary
        lines.append("## Summary")
        lines.append("")
        lines.append("### Key Metrics")
        lines.append("")
        lines.append("- **Compression Ratio**: Higher is better (original size / compressed size)")
        lines.append("- **Throughput**: Higher is better (MB processed per second)")
        lines.append("- **Latency**: Lower is better (milliseconds)")
        lines.append("- **Memory**: Lower is better (peak memory usage in MB)")
        lines.append("- **MSE**: Lower is better (reconstruction quality)")
        lines.append("- **96B Target**: Frackture should maintain ~96-byte payloads")
        lines.append("")
        lines.append("### Enhanced Frackture Verification")
        lines.append("")
        lines.append("1. **Payload Size Validation**: Ensures Frackture maintains its ~96-byte promise")
        lines.append("2. **Reconstruction Quality**: MSE measurements for baseline vs optimized encoding")
        lines.append("3. **Optimization Impact**: Measures effectiveness of self-optimization")
        lines.append("4. **Determinism Testing**: Validates consistent output across multiple runs")
        lines.append("5. **Fault Injection**: Tests error handling and tamper detection")
        lines.append("")
        lines.append("### Frackture Core Advantages")
        lines.append("")
        lines.append("- Fixed-size output (~96 bytes) regardless of input size")
        lines.append("- Identity-preserving symbolic fingerprints")
        lines.append("- Fast hashing for integrity checks")
        lines.append("- Dual-channel (symbolic + entropy) encoding")
        lines.append("- Self-optimization with decoder feedback")
        lines.append("- Built-in fault detection and error handling")
        lines.append("")
        
        with open(output_path, 'w') as f:
            f.write('\n'.join(lines))
        
        print(f"âœ… Markdown results saved to: {output_path}")
        print(f"ðŸ“Š Enhanced verification metrics included")


def run_benchmark_suite(
    small_datasets: bool = True,
    large_datasets: bool = True,
    tiny_datasets: bool = True,
    extreme_datasets: bool = False,  # Disabled by default due to size
    all_tiers: bool = False,
    specific_tiers: Optional[List[str]] = None,
    include_huge: bool = False,
    specific_categories: Optional[List[str]] = None,
    output_dir: Path = None,
    use_real_datasets: bool = None,
    gzip_level: int = 6,
    brotli_quality: int = 6,
    gzip_levels: Optional[List[int]] = None,
    brotli_qualities: Optional[List[int]] = None
):
    """Run the complete benchmark suite with enhanced metrics and extreme dataset support"""
    
    if output_dir is None:
        output_dir = Path(__file__).parent / "results"
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Auto-detect if we should use real datasets
    if use_real_datasets is None:
        use_real_datasets = HAS_DATASET_REPO
    elif use_real_datasets and not HAS_DATASET_REPO:
        print("Warning: Real datasets requested but DatasetRepository not available")
        use_real_datasets = False
    
    def _dedupe_ints(values: List[int]) -> List[int]:
        seen = set()
        out: List[int] = []
        for v in values:
            if v not in seen:
                seen.add(v)
                out.append(v)
        return out

    if gzip_levels is None:
        gzip_levels = [gzip_level]
    if brotli_qualities is None:
        brotli_qualities = [brotli_quality]

    gzip_levels = _dedupe_ints(gzip_levels)
    brotli_qualities = _dedupe_ints(brotli_qualities)

    def _annotate_dataset_metadata(
        results: List[BenchmarkResult],
        *,
        tier_name: str,
        category_name: str,
        actual_size_bytes: int,
        target_size_bytes: int,
    ) -> None:
        for r in results:
            r.tier_name = tier_name
            r.category_name = category_name
            r.actual_size_bytes = actual_size_bytes
            r.target_size_bytes = target_size_bytes

    print("\n" + "="*120)
    print("ðŸ”¥ FRACKTURE ENHANCED BENCHMARK SUITE ðŸ”¥")
    print("="*120)
    print(f"\nðŸ“ Output directory: {output_dir}")
    print(f"ðŸ“Š Dataset mode: {'REAL DATASETS' if use_real_datasets else 'SYNTHETIC DATASETS'}")
    print(f"ðŸ§® Brotli available: {HAS_BROTLI}")
    print(f"ðŸ” Cryptography available: {HAS_CRYPTOGRAPHY}")
    print(f"ðŸ’¾ Psutil available: {HAS_PSUTIL}")
    print(f"ðŸ“ Tiny datasets: {'âœ…' if tiny_datasets else 'âŒ'}")
    print(f"ðŸš€ Extreme datasets: {'âœ…' if extreme_datasets else 'âŒ'}")
    print(f"ðŸ—œï¸  Gzip levels: {gzip_levels}")
    print(f"ðŸ—œï¸  Brotli qualities: {brotli_qualities}")
    print(f"âœ¨ Enhanced metrics: Payload sizing, MSE, optimization, determinism, fault injection")
    
    all_results = {}
    
    # Initialize dataset repository if using real datasets
    repo = None
    if use_real_datasets:
        try:
            repo = DatasetRepository()
            print(f"âœ“ Loaded {len(repo.list_datasets())} real datasets from manifest")
        except Exception as e:
            print(f"âœ— Failed to load DatasetRepository: {e}")
            print("  Falling back to synthetic datasets")
            use_real_datasets = False
    
    # Run benchmarks on small datasets
    if small_datasets:
        print("\nðŸ“Š Running benchmarks on small datasets (100KB)...")
        
        if use_real_datasets:
            # Get all real datasets at medium tier (100KB)
            try:
                datasets = repo.get_all_datasets(tier='medium', skip_optional=True)
                print(f"  Loaded {len(datasets)} real dataset samples")
            except Exception as e:
                print(f"  Error loading real datasets: {e}")
                datasets = DatasetGenerator.get_all_datasets(small=True)
        else:
            datasets = DatasetGenerator.get_all_datasets(small=True)
        
        for dataset_name, data in datasets.items():
            print(f"\nðŸ” Benchmarking dataset: {dataset_name}")
            results = []
            
            # Frackture
            print("  - Running Frackture...")
            result = BenchmarkRunner.benchmark_frackture(data)
            result.dataset_type = dataset_name
            results.append(result)

            # SHA256
            print("  - Running SHA256...")
            result = BenchmarkRunner.benchmark_sha256(data)
            result.dataset_type = dataset_name
            results.append(result)

            # AES-GCM
            print("  - Running AES-GCM...")
            result = BenchmarkRunner.benchmark_aes_gcm(data)
            result.dataset_type = dataset_name
            results.append(result)

            # Frackture Encryption
            print("  - Running Frackture Encryption...")
            result = BenchmarkRunner.benchmark_frackture_encryption(data)
            result.dataset_type = dataset_name
            results.append(result)
            
            # Gzip
            for level in gzip_levels:
                print(f"  - Running Gzip (L{level})...")
                result = BenchmarkRunner.benchmark_gzip(data, level=level)
                result.dataset_type = dataset_name
                results.append(result)
            
            # Brotli
            if HAS_BROTLI:
                for quality in brotli_qualities:
                    print(f"  - Running Brotli (Q{quality})...")
                    result = BenchmarkRunner.benchmark_brotli(data, quality=quality)
                    result.dataset_type = dataset_name
                    results.append(result)

            category_name = "synthetic"
            target_size_bytes = 100 * 1024
            if use_real_datasets and repo and dataset_name in repo.datasets:
                category_name = repo.datasets[dataset_name].category
                target_size_bytes = repo.size_tiers['medium'].target

            _annotate_dataset_metadata(
                results,
                tier_name="medium",
                category_name=category_name,
                actual_size_bytes=len(data),
                target_size_bytes=target_size_bytes,
            )

            all_results[f"small_{dataset_name}"] = results
            ResultFormatter.print_table(results, f"small_{dataset_name}")
    
    # Run benchmarks on large datasets
    if large_datasets:
        print("\nðŸ“Š Running benchmarks on large datasets (1MB)...")
        
        if use_real_datasets:
            # Get all real datasets at large tier (1MB)
            try:
                datasets = repo.get_all_datasets(tier='large', skip_optional=True)
                print(f"  Loaded {len(datasets)} real dataset samples")
            except Exception as e:
                print(f"  Error loading real datasets: {e}")
                datasets = DatasetGenerator.get_all_datasets(small=False)
        else:
            datasets = DatasetGenerator.get_all_datasets(small=False)
        
        for dataset_name, data in datasets.items():
            print(f"\nðŸ” Benchmarking dataset: {dataset_name}")
            results = []
            
            # Frackture
            print("  - Running Frackture...")
            result = BenchmarkRunner.benchmark_frackture(data)
            result.dataset_type = dataset_name
            results.append(result)
            
            # SHA256
            print("  - Running SHA256...")
            result = BenchmarkRunner.benchmark_sha256(data)
            result.dataset_type = dataset_name
            results.append(result)

            # AES-GCM
            print("  - Running AES-GCM...")
            result = BenchmarkRunner.benchmark_aes_gcm(data)
            result.dataset_type = dataset_name
            results.append(result)

            # Frackture Encryption
            print("  - Running Frackture Encryption...")
            result = BenchmarkRunner.benchmark_frackture_encryption(data)
            result.dataset_type = dataset_name
            results.append(result)

            # Gzip
            for level in gzip_levels:
                print(f"  - Running Gzip (L{level})...")
                result = BenchmarkRunner.benchmark_gzip(data, level=level)
                result.dataset_type = dataset_name
                results.append(result)
            
            # Brotli
            if HAS_BROTLI:
                for quality in brotli_qualities:
                    print(f"  - Running Brotli (Q{quality})...")
                    result = BenchmarkRunner.benchmark_brotli(data, quality=quality)
                    result.dataset_type = dataset_name
                    results.append(result)

            category_name = "synthetic"
            target_size_bytes = 1 * 1024 * 1024
            if use_real_datasets and repo and dataset_name in repo.datasets:
                category_name = repo.datasets[dataset_name].category
                target_size_bytes = repo.size_tiers['large'].target

            _annotate_dataset_metadata(
                results,
                tier_name="large",
                category_name=category_name,
                actual_size_bytes=len(data),
                target_size_bytes=target_size_bytes,
            )

            all_results[f"large_{dataset_name}"] = results
            ResultFormatter.print_table(results, f"large_{dataset_name}")
    
    # Run benchmarks on tiny datasets (<100B)
    if tiny_datasets:
        print("\nðŸ“ Running benchmarks on tiny datasets (<100B)...")
        datasets = DatasetGenerator.generate_tiny_datasets()
        print(f"  Generated {len(datasets)} tiny dataset samples")
        
        for dataset_name, data in datasets.items():
            print(f"\nðŸ” Benchmarking tiny dataset: {dataset_name}")
            results = []
            
            # Frackture
            print("  - Running Frackture...")
            result = BenchmarkRunner.benchmark_frackture(data)
            result.dataset_type = dataset_name
            results.append(result)

            # SHA256
            print("  - Running SHA256...")
            result = BenchmarkRunner.benchmark_sha256(data)
            result.dataset_type = dataset_name
            results.append(result)

            # AES-GCM
            print("  - Running AES-GCM...")
            result = BenchmarkRunner.benchmark_aes_gcm(data)
            result.dataset_type = dataset_name
            results.append(result)

            # Frackture Encryption
            print("  - Running Frackture Encryption...")
            result = BenchmarkRunner.benchmark_frackture_encryption(data)
            result.dataset_type = dataset_name
            results.append(result)
            
            # Gzip (only for non-empty data)
            if len(data) > 0:
                for level in gzip_levels:
                    print(f"  - Running Gzip (L{level})...")
                    result = BenchmarkRunner.benchmark_gzip(data, level=level)
                    result.dataset_type = dataset_name
                    results.append(result)
            
            # Brotli (only for non-empty data)
            if HAS_BROTLI and len(data) > 0:
                for quality in brotli_qualities:
                    print(f"  - Running Brotli (Q{quality})...")
                    result = BenchmarkRunner.benchmark_brotli(data, quality=quality)
                    result.dataset_type = dataset_name
                    results.append(result)

            _annotate_dataset_metadata(
                results,
                tier_name="tiny",
                category_name="synthetic",
                actual_size_bytes=len(data),
                target_size_bytes=len(data),
            )

            all_results[f"tiny_{dataset_name}"] = results
            ResultFormatter.print_table(results, f"tiny_{dataset_name}")
    
    # Run benchmarks on extreme datasets (>100MB)
    if extreme_datasets:
        print("\nðŸš€ Running benchmarks on extreme datasets (>100MB)...")
        print("âš ï¸  WARNING: These tests may take a very long time and consume significant memory!")
        
        datasets = DatasetGenerator.generate_extreme_datasets()
        print(f"  Generated {len(datasets)} extreme dataset samples")
        
        for dataset_name, data in datasets.items():
            size_mb = len(data) / (1024 * 1024)
            print(f"\nðŸ” Benchmarking extreme dataset: {dataset_name} ({size_mb:.1f}MB)")
            results = []
            
            # Frackture (always included)
            print("  - Running Frackture...")
            result = BenchmarkRunner.benchmark_frackture(data)
            result.dataset_type = dataset_name
            results.append(result)

            # SHA256
            print("  - Running SHA256...")
            result = BenchmarkRunner.benchmark_sha256(data)
            result.dataset_type = dataset_name
            results.append(result)

            # AES-GCM
            print("  - Running AES-GCM...")
            result = BenchmarkRunner.benchmark_aes_gcm(data)
            result.dataset_type = dataset_name
            results.append(result)

            # Frackture Encryption
            print("  - Running Frackture Encryption...")
            result = BenchmarkRunner.benchmark_frackture_encryption(data)
            result.dataset_type = dataset_name
            results.append(result)
            
            # Only test gzip on smaller extreme datasets to avoid timeout
            if size_mb <= 10:  # Only test datasets <= 10MB with gzip
                for level in gzip_levels:
                    print(f"  - Running Gzip (L{level})...")
                    result = BenchmarkRunner.benchmark_gzip(data, level=level)
                    result.dataset_type = dataset_name
                    results.append(result)
            else:
                print("  - Skipping Gzip (dataset too large)")
            
            # Skip Brotli for extreme datasets due to performance
            print("  - Skipping Brotli (performance reasons)")

            _annotate_dataset_metadata(
                results,
                tier_name="extreme",
                category_name="synthetic",
                actual_size_bytes=len(data),
                target_size_bytes=len(data),
            )

            all_results[f"extreme_{dataset_name}"] = results
            ResultFormatter.print_table(results, f"extreme_{dataset_name}")
    
    # Run full-tier benchmarks (all categories Ã— all tiers)
    if all_tiers or specific_tiers:
        print("\nðŸŽ¯ Running full-tier benchmarks across all categories and size tiers...")
        
        if not use_real_datasets:
            print("âœ— Full-tier benchmarks require real datasets")
            print("  Falling back to synthetic datasets")
            all_tiers = False
            specific_tiers = None
        elif not repo:
            print("âœ— Dataset repository not available")
            all_tiers = False
            specific_tiers = None
        else:
            try:
                # Determine which tiers to run
                if all_tiers:
                    tier_names = repo.list_size_tiers()
                    if not include_huge:
                        tier_names = [t for t in tier_names if not repo.size_tiers[t].optional]
                elif specific_tiers:
                    tier_names = []
                    for tier in specific_tiers:
                        if tier in repo.size_tiers:
                            if not repo.size_tiers[tier].optional or include_huge:
                                tier_names.append(tier)
                        else:
                            print(f"  âš ï¸  Unknown tier: {tier}, skipping")
                
                # Determine which categories to run
                if specific_categories:
                    category_names = []
                    available_categories = set(repo.list_categories())
                    for category in specific_categories:
                        if category in available_categories:
                            category_names.append(category)
                        else:
                            print(f"  âš ï¸  Unknown category: {category}, skipping")
                else:
                    category_names = repo.list_categories()
                
                print(f"  ðŸ“ Running tiers: {tier_names}")
                print(f"  ðŸ“‚ Running categories: {category_names}")
                print(f"  ðŸŽ¯ Total combinations: {len(tier_names)} tiers Ã— {len(category_names)} categories = {len(tier_names) * len(category_names)} tests")
                
                # Track statistics
                total_tests = 0
                successful_tests = 0
                failed_tests = 0
                
                for tier_name in tier_names:
                    tier_info = repo.size_tiers[tier_name]
                    print(f"\nðŸ“Š Processing tier: {tier_name} (target: {tier_info.target:,} bytes)")
                    
                    for category_name in category_names:
                        # Get datasets in this category
                        dataset_names = repo.get_datasets_by_category(category_name)
                        if not dataset_names:
                            print(f"  âš ï¸  No datasets found for category: {category_name}")
                            continue
                        
                        for dataset_name in dataset_names:
                            try:
                                # Skip optional datasets that don't exist
                                dataset_info = repo.datasets[dataset_name]
                                if dataset_info.optional:
                                    file_path = repo.datasets_dir / dataset_info.file
                                    if not file_path.exists():
                                        continue
                                
                                total_tests += 1
                                
                                # Load dataset at this tier
                                data = repo.load_by_tier(dataset_name, tier_name)
                                
                                # Create tiered dataset name
                                tiered_dataset_name = f"{dataset_name}_{tier_name}"
                                actual_size = len(data)
                                
                                print(f"\nðŸ” Benchmarking {category_name}/{tier_name}: {dataset_name} ({actual_size:,} bytes)")
                                results = []
                                
                                # Run Frackture (always included)
                                print("  - Running Frackture...")
                                result = BenchmarkRunner.benchmark_frackture(data)
                                result.dataset_type = tiered_dataset_name
                                results.append(result)
                                
                                if result.success:
                                    successful_tests += 1
                                else:
                                    failed_tests += 1
                                    print(f"    âŒ Frackture failed: {result.error}")
                                
                                # Run SHA256
                                print("  - Running SHA256...")
                                result = BenchmarkRunner.benchmark_sha256(data)
                                result.dataset_type = tiered_dataset_name
                                results.append(result)
                                
                                # Run AES-GCM
                                print("  - Running AES-GCM...")
                                result = BenchmarkRunner.benchmark_aes_gcm(data)
                                result.dataset_type = tiered_dataset_name
                                results.append(result)
                                
                                # Run Frackture Encryption
                                print("  - Running Frackture Encryption...")
                                result = BenchmarkRunner.benchmark_frackture_encryption(data)
                                result.dataset_type = tiered_dataset_name
                                results.append(result)
                                
                                # Run Gzip (skip for huge datasets to avoid timeouts)
                                if tier_name != 'huge' and actual_size <= 50 * 1024 * 1024:  # Skip > 50MB
                                    for level in gzip_levels:
                                        print(f"  - Running Gzip (L{level})...")
                                        result = BenchmarkRunner.benchmark_gzip(data, level=level)
                                        result.dataset_type = tiered_dataset_name
                                        results.append(result)
                                else:
                                    print(f"  - Skipping Gzip (tier too large: {tier_name})")
                                
                                # Run Brotli (only for small/medium datasets due to performance)
                                if HAS_BROTLI and tier_info.max <= 10 * 1024 * 1024:  # Skip > 10MB
                                    for quality in brotli_qualities:
                                        print(f"  - Running Brotli (Q{quality})...")
                                        result = BenchmarkRunner.benchmark_brotli(data, quality=quality)
                                        result.dataset_type = tiered_dataset_name
                                        results.append(result)
                                else:
                                    print(f"  - Skipping Brotli (tier too large: {tier_name})")
                                
                                _annotate_dataset_metadata(
                                    results,
                                    tier_name=tier_name,
                                    category_name=category_name,
                                    actual_size_bytes=actual_size,
                                    target_size_bytes=tier_info.target,
                                )

                                # Store results with tiered naming
                                all_results[tiered_dataset_name] = results

                                # Print summary table for this dataset
                                ResultFormatter.print_table(results, tiered_dataset_name)
                                
                            except Exception as e:
                                failed_tests += 1
                                print(f"  âŒ Failed to benchmark {dataset_name} at {tier_name}: {e}")
                                continue
                
                # Print summary
                print(f"\nðŸŽ¯ Full-tier benchmarks completed:")
                print(f"  ðŸ“Š Total tests: {total_tests}")
                print(f"  âœ… Successful: {successful_tests}")
                print(f"  âŒ Failed: {failed_tests}")
                print(f"  ðŸ“ˆ Success rate: {(successful_tests/total_tests*100) if total_tests > 0 else 0:.1f}%")
                
            except Exception as e:
                print(f"âœ— Error in full-tier benchmarks: {e}")
                import traceback
                traceback.print_exc()
    
    def _init_competition_bucket() -> Dict[str, Any]:
        return {
            "total_comparisons": 0,
            "frackture_wins_ratio": 0,
            "frackture_losses_ratio": 0,
            "frackture_ties_ratio": 0,
            "frackture_wins_throughput": 0,
            "frackture_losses_throughput": 0,
            "frackture_ties_throughput": 0,
            "gzip_by_level": {},
            "brotli_by_quality": {},
            "win_rate_ratio": 0.0,
            "win_rate_throughput": 0.0,
        }

    def _finalize_bucket(bucket: Dict[str, Any]) -> Dict[str, Any]:
        total = bucket.get("total_comparisons", 0) or 0
        if total > 0:
            bucket["win_rate_ratio"] = bucket.get("frackture_wins_ratio", 0) / total
            bucket["win_rate_throughput"] = bucket.get("frackture_wins_throughput", 0) / total
        else:
            bucket["win_rate_ratio"] = 0.0
            bucket["win_rate_throughput"] = 0.0
        return bucket

    def _update_bucket(bucket: Dict[str, Any], *, ratio_win: Optional[bool], throughput_win: Optional[bool]) -> None:
        bucket["total_comparisons"] += 1

        if ratio_win is True:
            bucket["frackture_wins_ratio"] += 1
        elif ratio_win is False:
            bucket["frackture_losses_ratio"] += 1
        else:
            bucket["frackture_ties_ratio"] += 1

        if throughput_win is True:
            bucket["frackture_wins_throughput"] += 1
        elif throughput_win is False:
            bucket["frackture_losses_throughput"] += 1
        else:
            bucket["frackture_ties_throughput"] += 1

    competition_records: Dict[str, Any] = {}
    competition_summary: Dict[str, Any] = {
        "overall": _init_competition_bucket(),
        "by_tier": {},
    }

    for dataset_key, results in all_results.items():
        frackture_result = next((r for r in results if r.name == "Frackture" and r.success), None)
        if not frackture_result:
            continue

        tier_name = frackture_result.tier_name or "unknown"
        category_name = frackture_result.category_name or "unknown"

        dataset_record = {
            "dataset": dataset_key,
            "tier_name": tier_name,
            "category_name": category_name,
            "frackture": {
                "compression_ratio": frackture_result.compression_ratio,
                "encode_throughput": frackture_result.encode_throughput,
                "original_size": frackture_result.original_size,
            },
            "comparisons": [],
        }

        tier_bucket = competition_summary["by_tier"].setdefault(tier_name, _init_competition_bucket())

        for competitor in results:
            if not competitor.success:
                continue
            if competitor.gzip_level is None and competitor.brotli_quality is None:
                continue

            ratio_win: Optional[bool]
            throughput_win: Optional[bool]

            if abs(frackture_result.compression_ratio - competitor.compression_ratio) < 1e-12:
                ratio_win = None
            else:
                ratio_win = frackture_result.compression_ratio > competitor.compression_ratio

            if abs(frackture_result.encode_throughput - competitor.encode_throughput) < 1e-12:
                throughput_win = None
            else:
                throughput_win = frackture_result.encode_throughput > competitor.encode_throughput

            comparison = {
                "competitor": competitor.name,
                "gzip_level": competitor.gzip_level,
                "brotli_quality": competitor.brotli_quality,
                "competitor_compression_ratio": competitor.compression_ratio,
                "competitor_encode_throughput": competitor.encode_throughput,
                "frackture_wins_ratio": ratio_win,
                "frackture_wins_throughput": throughput_win,
            }
            dataset_record["comparisons"].append(comparison)

            _update_bucket(competition_summary["overall"], ratio_win=ratio_win, throughput_win=throughput_win)
            _update_bucket(tier_bucket, ratio_win=ratio_win, throughput_win=throughput_win)

            if competitor.gzip_level is not None:
                key = str(competitor.gzip_level)
                cfg_bucket = competition_summary["overall"]["gzip_by_level"].setdefault(key, _init_competition_bucket())
                _update_bucket(cfg_bucket, ratio_win=ratio_win, throughput_win=throughput_win)
            if competitor.brotli_quality is not None:
                key = str(competitor.brotli_quality)
                cfg_bucket = competition_summary["overall"]["brotli_by_quality"].setdefault(key, _init_competition_bucket())
                _update_bucket(cfg_bucket, ratio_win=ratio_win, throughput_win=throughput_win)

        competition_records[dataset_key] = dataset_record

    _finalize_bucket(competition_summary["overall"])
    for tier_name, bucket in competition_summary["by_tier"].items():
        _finalize_bucket(bucket)
    for bucket in competition_summary["overall"]["gzip_by_level"].values():
        _finalize_bucket(bucket)
    for bucket in competition_summary["overall"]["brotli_by_quality"].values():
        _finalize_bucket(bucket)

    # Save results
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    json_path = output_dir / f"benchmark_results_{timestamp}.json"
    md_path = output_dir / f"benchmark_results_{timestamp}.md"

    benchmark_config = {
        "gzip_levels": gzip_levels,
        "brotli_qualities": brotli_qualities,
        "use_real_datasets": use_real_datasets,
        "all_tiers": all_tiers,
        "specific_tiers": specific_tiers,
        "include_huge": include_huge,
        "specific_categories": specific_categories,
        "small_datasets": small_datasets,
        "large_datasets": large_datasets,
        "tiny_datasets": tiny_datasets,
        "extreme_datasets": extreme_datasets,
    }

    ResultFormatter.save_json(
        all_results,
        json_path,
        competition_summary=competition_summary,
        competition_records=competition_records,
        benchmark_config=benchmark_config,
    )
    ResultFormatter.save_markdown(
        all_results,
        md_path,
        competition_summary=competition_summary,
        benchmark_config=benchmark_config,
    )

    print("\n" + "="*100)
    print("âœ… BENCHMARK SUITE COMPLETED")
    print("="*100)
    print(f"\nResults saved to:")
    print(f"  - JSON: {json_path}")
    print(f"  - Markdown: {md_path}")
    print()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Frackture Enhanced Benchmark Suite with Verification Metrics",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s                           # Run all benchmarks (small, large, tiny)
  %(prog)s --small-only             # Run only 100KB dataset benchmarks
  %(prog)s --large-only             # Run only 1MB dataset benchmarks  
  %(prog)s --tiny-only              # Run only <100B dataset benchmarks
  %(prog)s --extreme                # Run extreme >100MB dataset benchmarks
  %(prog)s --no-tiny               # Disable tiny dataset tests
  
  %(prog)s --all-tiers             # Run all tiers across all categories
  %(prog)s --all-tiers --categories text,binary  # All tiers, specific categories
  %(prog)s --tiers tiny,small,medium # Specific tier subset
  %(prog)s --all-tiers --include-huge  # Include 1GB+ tier
  
  %(prog)s --synthetic             # Use synthetic datasets only
  %(prog)s --real                  # Use real datasets if available
  %(prog)s --output-dir ./results  # Custom output directory
        """
    )
    
    # Dataset size options
    dataset_group = parser.add_mutually_exclusive_group()
    dataset_group.add_argument(
        "--small-only",
        action="store_true",
        help="Run only small dataset benchmarks (100KB)"
    )
    dataset_group.add_argument(
        "--large-only", 
        action="store_true",
        help="Run only large dataset benchmarks (1MB)"
    )
    dataset_group.add_argument(
        "--tiny-only",
        action="store_true", 
        help="Run only tiny dataset benchmarks (<100B)"
    )
    dataset_group.add_argument(
        "--extreme-only",
        action="store_true",
        help="Run only extreme dataset benchmarks (>100MB)"
    )
    dataset_group.add_argument(
        "--all-tiers",
        action="store_true",
        help="Run benchmarks across all size tiers (tiny â†’ huge) for real datasets"
    )
    dataset_group.add_argument(
        "--tiers",
        type=str,
        help="Run benchmarks on specific tiers (e.g., 'tiny,small,medium,large')"
    )
    
    # Additional dataset options
    parser.add_argument(
        "--no-tiny",
        action="store_true",
        help="Disable tiny dataset benchmarks (<100B)"
    )
    parser.add_argument(
        "--extreme",
        action="store_true",
        help="Enable extreme dataset benchmarks (>100MB, can be very slow)"
    )
    parser.add_argument(
        "--include-huge",
        action="store_true",
        help="Include the huge (1GB+) tier in full-tier benchmarks (optional)"
    )
    parser.add_argument(
        "--categories",
        type=str,
        help="Run benchmarks on specific categories (e.g., 'text,binary,code,structured,mixed')"
    )

    parser.add_argument(
        "--competition-report",
        action="store_true",
        help="Run all real dataset tiers and sweep gzip/brotli configurations to generate a comprehensive competition_summary (win/loss rates by tier)"
    )
    
    # Output and mode options
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=None,
        help="Output directory for results (default: benchmarks/results)"
    )
    parser.add_argument(
        "--synthetic",
        action="store_true",
        help="Use synthetic datasets instead of real samples (legacy mode)"
    )
    parser.add_argument(
        "--real",
        action="store_true",
        help="Use real dataset samples (default if available)"
    )
    
    # Enhanced features
    parser.add_argument(
        "--verify-only",
        action="store_true",
        help="Run only verification metrics (payload sizing, MSE, determinism, fault injection)"
    )
    parser.add_argument(
        "--detailed",
        action="store_true",
        help="Enable detailed verification output with more diagnostic information"
    )
    
    # Compression settings
    parser.add_argument(
        "--gzip-level",
        type=int,
        default=6,
        choices=range(1, 10),
        help="Gzip compression level (1-9)"
    )
    parser.add_argument(
        "--gzip-levels",
        type=int,
        nargs='+',
        choices=range(1, 10),
        help="Gzip compression levels to sweep in a single run (e.g. --gzip-levels 1 6 9). Overrides --gzip-level."
    )
    parser.add_argument(
        "--brotli-quality",
        type=int,
        default=6,
        choices=range(0, 12),
        help="Brotli compression quality (0-11)"
    )
    parser.add_argument(
        "--brotli-qualities",
        type=int,
        nargs='+',
        choices=range(0, 12),
        help="Brotli compression qualities to sweep in a single run (e.g. --brotli-qualities 4 6 11). Overrides --brotli-quality."
    )
    
    args = parser.parse_args()

    DEFAULT_GZIP_LEVELS = [1, 6, 9]
    DEFAULT_BROTLI_QUALITIES = [4, 6, 11]

    def _flag_present(flag: str) -> bool:
        return flag in sys.argv

    if args.competition_report:
        gzip_levels = list(range(1, 10))
        brotli_qualities = list(range(0, 12))
    else:
        gzip_levels = args.gzip_levels if args.gzip_levels is not None else (
            [args.gzip_level] if _flag_present("--gzip-level") else DEFAULT_GZIP_LEVELS
        )
        brotli_qualities = args.brotli_qualities if args.brotli_qualities is not None else (
            [args.brotli_quality] if _flag_present("--brotli-quality") else DEFAULT_BROTLI_QUALITIES
        )

    # Parse tier selections
    all_tiers = args.all_tiers or args.competition_report
    specific_tiers = None
    if args.tiers and not args.competition_report:
        specific_tiers = [t.strip() for t in args.tiers.split(',')]
    include_huge = args.include_huge

    # Parse category selections
    specific_categories = None
    if args.categories:
        specific_categories = [c.strip() for c in args.categories.split(',')]

    # Determine which datasets to run
    if args.competition_report:
        small = False
        large = False
        tiny = False
        extreme = False
    else:
        small = not (args.large_only or args.tiny_only or args.extreme_only or args.all_tiers or args.tiers)
        large = not (args.small_only or args.tiny_only or args.extreme_only or args.all_tiers or args.tiers)
        tiny = not (args.small_only or args.large_only or args.extreme_only or args.all_tiers or args.tiers) and not args.no_tiny
        extreme = args.extreme_only or args.extreme

    # Determine dataset mode
    use_real = None  # Auto-detect
    if args.synthetic:
        use_real = False
    elif args.real or all_tiers or specific_tiers or args.competition_report:
        use_real = True  # Tier-based benchmarks require real datasets
    
    print("\nðŸš€ Enhanced Frackture Benchmark Configuration:")
    print(f"  Small datasets (100KB): {'âœ…' if small else 'âŒ'}")
    print(f"  Large datasets (1MB): {'âœ…' if large else 'âŒ'}")
    print(f"  Tiny datasets (<100B): {'âœ…' if tiny else 'âŒ'}")
    print(f"  Extreme datasets (>100MB): {'âœ…' if extreme else 'âŒ'}")
    if all_tiers:
        print(f"  All tiers mode: âœ… (FULL-TIER BENCHMARKS)")
        print(f"  Include huge tier: {'âœ…' if include_huge else 'âŒ'}")
    elif specific_tiers:
        print(f"  Specific tiers mode: âœ…")
        print(f"  Tiers: {specific_tiers}")
        print(f"  Include huge tier: {'âœ…' if include_huge else 'âŒ'}")
    else:
        print(f"  All tiers mode: âŒ")
    if specific_categories:
        print(f"  Specific categories: {specific_categories}")
    print(f"  Real datasets: {'âœ…' if use_real else 'Synthetic' if use_real is False else 'Auto-detect'}")
    print(f"  Enhanced verification: âœ…")
    print(f"  Gzip Levels: {gzip_levels}")
    print(f"  Brotli Qualities: {brotli_qualities}")
    
    run_benchmark_suite(
        small_datasets=small,
        large_datasets=large,
        tiny_datasets=tiny,
        extreme_datasets=extreme,
        all_tiers=all_tiers,
        specific_tiers=specific_tiers,
        include_huge=include_huge,
        specific_categories=specific_categories,
        output_dir=args.output_dir,
        use_real_datasets=use_real,
        gzip_level=args.gzip_level,
        brotli_quality=args.brotli_quality,
        gzip_levels=gzip_levels,
        brotli_qualities=brotli_qualities
    )
